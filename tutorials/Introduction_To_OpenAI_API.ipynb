{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Understanding AI with Open AI SDK\n",
    "\n",
    "The Open AI SDK provides a powerful set of tools for developers to implement artificial intelligence (AI) functionality into their applications. This guide will provide an overview of the SDK and its key features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before using the Open AI SDK, developers must first set up their environment. This includes installing the necessary dependencies and creating an API key. Once the environment is set up, developers can begin using the SDK to integrate AI functionality into their applications."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Account\n",
    "The OpenAI API requires API keys for authentication. To obtain your API key, you can visit the API Keys page. It is crucial to treat your API key as a secret and avoid sharing it with others or exposing it in client-side code like browsers or apps. \n",
    "\n",
    "When making API requests, remember to include your API key in the Authorization HTTP header using the format:\n",
    "```\n",
    "\"Authorization: Bearer OPENAI_API_KEY\".\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "\n",
    "We will be using the python sdk that Open AI has provided. To get started run the following command\n",
    "```\n",
    "pip install openai\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT\n",
    "GPT, which stands for generative pre-trained transformer, is a type of model developed by OpenAI. These models are trained to understand natural language and code, and they generate text outputs in response to given prompts. By providing instructions or examples in the prompts, GPT models can perform tasks such as content generation, code generation, summarization, conversation, and creative writing.\n",
    "\n",
    "### Embeddings\n",
    "Embeddings are vector representations of data, such as text, designed to capture and preserve content and meaning. OpenAI provides text embedding models that take text as input and produce embedding vectors as output. These embeddings are useful for a variety of applications, including search, clustering, recommendations, anomaly detection, and classification.\n",
    "\n",
    "### Tokens\n",
    "GPT and embedding models process text in chunks known as tokens. Tokens represent commonly occurring sequences of characters, and they enable the models to understand and process the text. For example, words like \"tokenization\" can be broken down into tokens like \"token\" and \"ization.\" Tokens play a role in determining the input and output constraints of the models, with the maximum context length defining the limit of prompt and generated output length for GPT models and input length for embedding models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Level Architecture\n",
    "This diagram provides a visual representation of how a language model processes input and generates output. It is a high-level depiction, and the actual process involves a lot more complexity, particularly within the attention layers and the generator.\n",
    "A simple way to look at the GPT Architecture\n",
    "\n",
    "```mermaid\n",
    "sequenceDiagram\n",
    "    participant U as User\n",
    "    participant T as Tokenizer\n",
    "    participant E as Embedder\n",
    "    participant A as Attention Layers\n",
    "    participant G as Generator\n",
    "    U->>T: Inputs text prompt\n",
    "    T->>E: Tokenizes text\n",
    "    E->>A: Embeds tokens into vectors\n",
    "    A->>A: Processes through attention layers\n",
    "    A->>G: Final layer outputs to Generator\n",
    "    G->>U: Outputs generated text\n",
    "```\n",
    "\n",
    "The user inputs a text prompt, which is then sent to the tokenizer. Tokenizer takes the prompt, breaks it up into tokens which are fed to the embedder. The embedder takes the tokens anc converts them into word vectors. Mathematically, the mapped into  a vector space where each dimension has some semantic meaning. That means some words can relate to others by some dimension. The attention layers process these vectors to work out the understanding. It helps the model determine which parts of the input are important and how they relate to each other. After the last attention layer, the output is sent to the generator. It then converts them into text based on other parameters (e.g., temperature, frequency penalty, and top_p (nucleus sampling))."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API\n",
    "The models table lists the compatibility of various models with different endpoints of the OpenAI API. It includes information about the model names that can be used with each endpoint, such as gpt-4, gpt-3.5-turbo, text-davinci-003, and more.\n",
    "### Models\n",
    "| ENDPOINT                    | MODEL NAME                      |\n",
    "|-----------------------------|---------------------------------|\n",
    "| /v1/chat/completions        | gpt-4, gpt-4-0314, gpt-4-32k, gpt-4-32k-0314, gpt-3.5-turbo, gpt-3.5-turbo-0301 |\n",
    "| /v1/completions             | text-davinci-003, text-davinci-002, text-curie-001, text-babbage-001, text-ada-001 |\n",
    "| /v1/edits                   | text-davinci-edit-001, code-davinci-edit-001 |\n",
    "| /v1/audio/transcriptions    | whisper-1                        |\n",
    "| /v1/audio/translations      | whisper-1                        |\n",
    "| /v1/fine-tunes              | davinci, curie, babbage, ada     |\n",
    "| /v1/embeddings              | text-embedding-ada-002, text-search-ada-doc-001 |\n",
    "| /v1/moderations             | text-moderation-stable, text-moderation-latest |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Usage\n",
    "As of March 1, 2023, OpenAI has implemented a data usage policy for its API. The policy states that any data sent to the OpenAI API will not be utilized for training or improving OpenAI models, unless the user explicitly opts in for such purposes.\n",
    "\n",
    "To prevent abuse, the API data may be stored for a maximum of 30 days, after which it will be permanently deleted, unless there are legal requirements for its retention. However, trusted customers who have sensitive applications can potentially benefit from zero data retention. In this case, request and response bodies are not stored in any logging mechanism and exist only temporarily in memory to fulfill the specific request.\n",
    "\n",
    "It's important to note that this data policy specifically pertains to the OpenAI API and does not apply to other OpenAI services meant for non-API consumers, such as ChatGPT or DALLÂ·E Labs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retention Policy for Data\n",
    "The data retention table outlines the default retention policies for different endpoints of the OpenAI API. It specifies whether the data used for training is retained, the default retention period (such as 30 days), and whether the endpoint is eligible for zero data retention. Some endpoints have zero data retention, meaning the data is not stored beyond the immediate request, while others have retention periods or retain data until it is deleted by the customer.\n",
    "\n",
    "| ENDPOINT                    | DATA USED FOR TRAINING          | DEFAULT RETENTION    | ELIGIBLE FOR ZERO RETENTION |\n",
    "|-----------------------------|---------------------------------|----------------------|----------------------------|\n",
    "| /v1/completions             | No                              | 30 days              | Yes                        |\n",
    "| /v1/chat/completions        | No                              | 30 days              | Yes                        |\n",
    "| /v1/edits                   | No                              | 30 days              | Yes                        |\n",
    "| /v1/images/generations      | No                              | 30 days              | No                         |\n",
    "| /v1/images/edits            | No                              | 30 days              | No                         |\n",
    "| /v1/images/variations       | No                              | 30 days              | No                         |\n",
    "| /v1/embeddings              | No                              | 30 days              | Yes                        |\n",
    "| /v1/audio/transcriptions    | No                              | Zero data retention | -                          |\n",
    "| /v1/audio/translations      | No                              | Zero data retention | -                          |\n",
    "| /v1/files                   | No                              | Until deleted by customer | No                         |\n",
    "| /v1/fine-tunes              | No                              | Until deleted by customer | No                         |\n",
    "| /v1/moderations             | No                              | Zero data retention | -                          |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a Request"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-7Np8nghpQrz1bhRCqBbRDUDI3pcDn', 'object': 'text_completion', 'created': 1685912485, 'model': 'text-davinci-003', 'choices': [{'text': '\\n\\nThis is indeed a test', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 5, 'completion_tokens': 7, 'total_tokens': 12}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "url = 'https://api.openai.com/v1/completions'\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': f'Bearer {OPENAI_API_KEY}'\n",
    "}\n",
    "data = {\n",
    "    'model': 'text-davinci-003',\n",
    "    'prompt': 'Say this is a test',\n",
    "    'max_tokens': 7,\n",
    "    'temperature': 0\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "print(response.json())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-7Np8MbJQ17hhsvI0I2YQ70gL7nkjW at 0x7fce5a0f3e90> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"length\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \"\\n\\nThis is indeed a test\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1685912458,\n",
       "  \"id\": \"cmpl-7Np8MbJQ17hhsvI0I2YQ70gL7nkjW\",\n",
       "  \"model\": \"text-davinci-003\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 7,\n",
       "    \"prompt_tokens\": 5,\n",
       "    \"total_tokens\": 12\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.Completion.create(\n",
    "  model=\"text-davinci-003\",\n",
    "  prompt=\"Say this is a test\",\n",
    "  max_tokens=7,\n",
    "  temperature=0\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completions\n",
    "Given a prompt, the model will return one or more predicted completions, and can also return the probabilities of alternative tokens at each position.\n",
    "\n",
    "\n",
    "### Completion Example (Parsing Unstructured Data)\n",
    "Create tables from long text by specifying the structure and supplying examples.\n",
    "#### Prompt\n",
    "```\n",
    "A table summarizing the fruits from Goocrux:\n",
    "\n",
    "There are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy. There are also loheckles, which are a grayish blue fruit and are very tart, a little bit like a lemon. Pounits are a bright green color and are more savory than sweet. There are also plenty of loopnovas which are a neon pink flavor and taste like cotton candy. Finally, there are fruits called glowls, which have a very sour and bitter taste which is acidic and caustic, and a pale orange tinge to them.\n",
    "\n",
    "| Fruit | Color | Flavor |\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| --- | --- | --- |\n",
      "| Neoskizzles | Purple | Candy |\n",
      "| Loheckles | Grayish Blue | Tart, like a lemon |\n",
      "| Pounits | Bright Green | Savory |\n",
      "| Loopnovas | Neon Pink | Cotton Candy |\n",
      "| Glowls | Pale Orange | Sour and Bitter, Acidic and Caustic |\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "response = openai.Completion.create(\n",
    "  model=\"text-davinci-003\",\n",
    "  prompt=\"A table summarizing the fruits from Goocrux:\\n\\nThere are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy. There are also loheckles, which are a grayish blue fruit and are very tart, a little bit like a lemon. Pounits are a bright green color and are more savory than sweet. There are also plenty of loopnovas which are a neon pink flavor and taste like cotton candy. Finally, there are fruits called glowls, which have a very sour and bitter taste which is acidic and caustic, and a pale orange tinge to them.\\n\\n| Fruit | Color | Flavor |\",\n",
    "  temperature=0,\n",
    "  max_tokens=100,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0\n",
    ")\n",
    "\n",
    "print(response.choices[0].text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion Example (Translate natural language to SQL queries.)\n",
    "Translate natural language to SQL queries.\n",
    "```\n",
    "#### Prompt\n",
    "### Postgres SQL tables, with their properties:\n",
    "#\n",
    "# Employee(id, name, department_id)\n",
    "# Department(id, name, address)\n",
    "# Salary_Payments(id, employee_id, amount, date)\n",
    "#\n",
    "### A query to list the names of the departments which employed more than 10 employees in the last 3 months\n",
    "SELECT\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT d.name \n",
      "FROM Department d \n",
      "INNER JOIN Employee e ON d.id = e.department_id \n",
      "INNER JOIN Salary_Payments sp ON e.id = sp.employee_id \n",
      "WHERE sp.date > NOW() - INTERVAL '3 months' \n",
      "GROUP BY d.name \n",
      "HAVING COUNT(*) > 10\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "response = openai.Completion.create(\n",
    "  model=\"text-davinci-003\",\n",
    "  prompt=\"### Postgres SQL tables, with their properties:\\n#\\n# Employee(id, name, department_id)\\n# Department(id, name, address)\\n# Salary_Payments(id, employee_id, amount, date)\\n#\\n### A query to list the names of the departments which employed more than 10 employees in the last 3 months\\n\",\n",
    "  temperature=0,\n",
    "  max_tokens=150,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0,\n",
    "  stop=[\"#\", \";\"]\n",
    ")\n",
    "\n",
    "print(response.choices[0].text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat\n",
    "\n",
    "### Prompt\n",
    "A prompt refers to the initial input or starting point for generating natural language text using the provided text-generating model.\n",
    "\n",
    "### Context Window\n",
    "\n",
    "a context window is a range of words or characters surrounding a particular word or sequence of words in a text. This window is used to provide additional context for a machine learning model to better understand the meaning of the target word or sequence.\n",
    "\n",
    "For example, consider the sentence \"I love to eat ice cream\". If we want to predict the next word after \"eat\", we can use a context window to provide additional context. A simple context window might be the surrounding words \"love to\" and \"ice cream\". This helps the model understand that \"eat\" in this context is likely associated with food or eating, rather than some other possible meanings of the word.\n",
    "\n",
    "The size of the context window can vary depending on the specific task and model being used. In some cases, a larger context window may be necessary to capture more complex relationships between words, while in other cases a smaller window may be sufficient.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Chat Bot with Context\n",
    "\n",
    "```mermaid\n",
    "sequenceDiagram\n",
    "    participant User\n",
    "    participant Model\n",
    "    User->>Model: Hello, how are you?\n",
    "    Model->>Model: Context window: \"Hello, how are you?\"\n",
    "    Model->>Model: Generating response\n",
    "    Model->>User: I'm doing well, thank you. How can I assist you?\n",
    "    User->>Model: I need help with my account.\n",
    "    Model->>Model: Context window: \"Hello, how are you? I'm doing well, thank you. How can I assist you?\"\n",
    "    Model->>Model: Generating response\n",
    "    Model->>User: Sure, what specifically do you need help with?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "# Set up OpenAI API credentials and model\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "conversation_history = []  # Empty list to store conversation history\n",
    "\n",
    "def chat(user_input: str):\n",
    "    # Example conversation loop\n",
    "    # Get input from user\n",
    "    print(\"User: \" + user_input)\n",
    "\n",
    "    # Append previous conversation to input prompt\n",
    "    prompt = \"Conversation history:\\n\" + \"\\n\".join(conversation_history) + \"\\n\\nUser: \" + user_input\n",
    "\n",
    "    # Generate response from OpenAI model\n",
    "    response = openai.Completion.create(\n",
    "        model=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=1024,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    # Extract response text\n",
    "    ai_response = response.choices[0].text.strip()\n",
    "\n",
    "    # Print AI response\n",
    "    print(\"AI:\", ai_response)\n",
    "\n",
    "    # Add user input and AI response to conversation history\n",
    "    conversation_history.append(\"User: \" + user_input)\n",
    "    conversation_history.append(\"AI: \" + ai_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hello how are you doing?\n",
      "AI: Bot: Hi there! I'm doing great, thanks for asking. How about you?\n"
     ]
    }
   ],
   "source": [
    "chat(\"Hello how are you doing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: I need help with my account\n",
      "AI: AI: What kind of help do you need with your account?\n"
     ]
    }
   ],
   "source": [
    "chat(\"I need help with my account.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we set up a conversation loop that prompts the user for input and generates a response using the OpenAI API. Before sending the prompt to the API, we append the entire conversation history up to that point to the prompt, with the new user input at the end. The AI response is then extracted from the API response and printed to the console, and the user input and AI response are added to the conversation history list."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat API and SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"content\": \"Hello there! How can I assist you today?\",\n",
      "  \"role\": \"assistant\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "  ]\n",
    ")\n",
    " \n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Ghost Writer Application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sections(title: str, tone: str, keywords: str) -> str:\n",
    "    prompt = f\"Generate three section headers for an article titled {title} in a {tone} tone \\\n",
    "              with a focus on {keywords}\"\n",
    "    return openai.Completion.create(\n",
    "        model=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        max_tokens: 150,\n",
    "        temperature: 0.8\n",
    "    ).choices[0].text.strip()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
