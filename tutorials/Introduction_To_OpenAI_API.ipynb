{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Understanding AI with Open AI SDK\n",
    "\n",
    "The Open AI SDK provides a powerful set of tools for developers to implement artificial intelligence (AI) functionality into their applications. This guide will provide an overview of the SDK and its key features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before using the Open AI SDK, developers must first set up their environment. This includes installing the necessary dependencies and creating an API key. Once the environment is set up, developers can begin using the SDK to integrate AI functionality into their applications."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Account\n",
    "The OpenAI API requires API keys for authentication. To obtain your API key, you can visit the API Keys page. It is crucial to treat your API key as a secret and avoid sharing it with others or exposing it in client-side code like browsers or apps. \n",
    "\n",
    "When making API requests, remember to include your API key in the Authorization HTTP header using the format:\n",
    "```\n",
    "\"Authorization: Bearer OPENAI_API_KEY\".\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "\n",
    "We will be using the python sdk that Open AI has provided. To get started run the following command\n",
    "```\n",
    "pip install openai\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API\n",
    "The models table lists the compatibility of various models with different endpoints of the OpenAI API. It includes information about the model names that can be used with each endpoint, such as gpt-4, gpt-3.5-turbo, text-davinci-003, and more.\n",
    "### Models\n",
    "| Endpoint                  | Model Name                                                                            | Description                                                                                               |\n",
    "|---------------------------|---------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------|\n",
    "| /v1/chat/completions      | gpt-4, gpt-4-0314, gpt-4-32k, gpt-4-32k-0314, gpt-3.5-turbo, gpt-3.5-turbo-0301         | Generates a chat-based response given a series of messages as input                                        |\n",
    "| /v1/completions           | text-davinci-003, text-davinci-002, text-curie-001, text-babbage-001, text-ada-001     | Generates a text completion or continuation given a prompt                                                |\n",
    "| /v1/edits                 | text-davinci-edit-001, code-davinci-edit-001                                           | Allows users to make edits or suggestions to improve OpenAI models' responses                             |\n",
    "| /v1/images/generations    | DALLE                                                        | Generates images based on textual descriptions or prompts                         |\n",
    "| /v1/images/edits| DALLE                                                        | Creates edited images based on textual descriptions or prompts and original image                         |\n",
    "| /v1/images/variations | DALLE                                                        | Creates variations of a given image                         |\n",
    "| /v1/audio/transcriptions  | whisper-1                                                                             | Converts spoken language audio to written text using the Whisper ASR system                              |\n",
    "| /v1/audio/translations    | whisper-1                                                                               | Converts spoken language audio into written text into English using the Whisper ASR system |\n",
    "| /v1/embeddings            | text-davinci-003, text-davinci-002, text-curie-001, text-babbage-001, text-ada-001     | Generates embeddings for a given text prompt or sentence                                                 |\n",
    "| /v1/fine-tunes            | davinci, curie, babbage, ada                                                           | Allows users to fine-tune OpenAI's base models on custom datasets                                         |\n",
    "| /v1/moderations           | content-moderator-alpha-c4, content-moderator-alpha-c4-robust-v1, content-moderator-gamma | Provides moderation of text content to help prevent the generation of content violating OpenAI's usage policy |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Usage\n",
    "As of March 1, 2023, OpenAI has implemented a data usage policy for its API. The policy states that any data sent to the OpenAI API will not be utilized for training or improving OpenAI models, unless the user explicitly opts in for such purposes.\n",
    "\n",
    "To prevent abuse, the API data may be stored for a maximum of 30 days, after which it will be permanently deleted, unless there are legal requirements for its retention. However, trusted customers who have sensitive applications can potentially benefit from zero data retention. In this case, request and response bodies are not stored in any logging mechanism and exist only temporarily in memory to fulfill the specific request.\n",
    "\n",
    "It's important to note that this data policy specifically pertains to the OpenAI API and does not apply to other OpenAI services meant for non-API consumers, such as ChatGPT or DALLÂ·E Labs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retention Policy for Data\n",
    "The data retention table outlines the default retention policies for different endpoints of the OpenAI API. It specifies whether the data used for training is retained, the default retention period (such as 30 days), and whether the endpoint is eligible for zero data retention. Some endpoints have zero data retention, meaning the data is not stored beyond the immediate request, while others have retention periods or retain data until it is deleted by the customer.\n",
    "\n",
    "| ENDPOINT                    | DATA USED FOR TRAINING          | DEFAULT RETENTION    | ELIGIBLE FOR ZERO RETENTION |\n",
    "|-----------------------------|---------------------------------|----------------------|----------------------------|\n",
    "| /v1/completions             | No                              | 30 days              | Yes                        |\n",
    "| /v1/chat/completions        | No                              | 30 days              | Yes                        |\n",
    "| /v1/edits                   | No                              | 30 days              | Yes                        |\n",
    "| /v1/images/generations      | No                              | 30 days              | No                         |\n",
    "| /v1/images/edits            | No                              | 30 days              | No                         |\n",
    "| /v1/images/variations       | No                              | 30 days              | No                         |\n",
    "| /v1/embeddings              | No                              | 30 days              | Yes                        |\n",
    "| /v1/audio/transcriptions    | No                              | Zero data retention | -                          |\n",
    "| /v1/audio/translations      | No                              | Zero data retention | -                          |\n",
    "| /v1/files                   | No                              | Until deleted by customer | No                         |\n",
    "| /v1/fine-tunes              | No                              | Until deleted by customer | No                         |\n",
    "| /v1/moderations             | No                              | Zero data retention | -                          |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a Request"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-7Np8nghpQrz1bhRCqBbRDUDI3pcDn', 'object': 'text_completion', 'created': 1685912485, 'model': 'text-davinci-003', 'choices': [{'text': '\\n\\nThis is indeed a test', 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 5, 'completion_tokens': 7, 'total_tokens': 12}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "url = 'https://api.openai.com/v1/completions'\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Authorization': f'Bearer {OPENAI_API_KEY}'\n",
    "}\n",
    "data = {\n",
    "    'model': 'text-davinci-003',\n",
    "    'prompt': 'Say this is a test',\n",
    "    'max_tokens': 7,\n",
    "    'temperature': 0\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "print(response.json())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-7Np8MbJQ17hhsvI0I2YQ70gL7nkjW at 0x7fce5a0f3e90> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"length\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \"\\n\\nThis is indeed a test\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1685912458,\n",
       "  \"id\": \"cmpl-7Np8MbJQ17hhsvI0I2YQ70gL7nkjW\",\n",
       "  \"model\": \"text-davinci-003\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 7,\n",
       "    \"prompt_tokens\": 5,\n",
       "    \"total_tokens\": 12\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.Completion.create(\n",
    "  model=\"text-davinci-003\",\n",
    "  prompt=\"Say this is a test\",\n",
    "  max_tokens=7,\n",
    "  temperature=0\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completions\n",
    "Given a prompt, the model will return one or more predicted completions, and can also return the probabilities of alternative tokens at each position.\n",
    "\n",
    "### Parameters\n",
    "| Parameter          | Type             | Description                                                                                                                            |\n",
    "|--------------------|------------------|----------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| model              | string           | Required. ID of the model to use.                                                                                                      |\n",
    "| prompt             | string or array  | Optional. The prompt(s) to generate completions for.                                                                                    |\n",
    "| suffix             | string           | Optional. The suffix that comes after a completion of inserted text.                                                                    |\n",
    "| max_tokens         | integer          | Optional. The maximum number of tokens to generate in the completion.                                                                   |\n",
    "| temperature        | number           | Optional. What sampling temperature to use, between 0 and 2.                                                                            |\n",
    "| top_p              | number           | Optional. An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass. |\n",
    "| n                  | integer          | Optional. How many completions to generate for each prompt.                                                                             |\n",
    "| stream             | boolean          | Optional. Whether to stream back partial progress.                                                                                      |\n",
    "| logprobs           | integer          | Optional. Include the log probabilities on the logprobs most likely tokens.                                                             |\n",
    "| echo               | boolean          | Optional. Echo back the prompt in addition to the completion.                                                                           |\n",
    "| stop               | string or array  | Optional. Up to 4 sequences where the API will stop generating further tokens.                                                          |\n",
    "| presence_penalty   | number           | Optional. Number between -2.0 and 2.0.                                                                                                 |\n",
    "| frequency_penalty  | number           | Optional. Number between -2.0 and 2.0.                                                                                                 |\n",
    "| best_of            | integer          | Optional. Generates best_of completions server-side and returns the \"best\".                                                             |\n",
    "| logit_bias         | map              | Optional. Modify the likelihood of specified tokens appearing in the completion.                                                        |\n",
    "| user               | string           | Optional. A unique identifier representing your end-user.                                                                               |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion Example (Parsing Unstructured Data)\n",
    "Create tables from long text by specifying the structure and supplying examples.\n",
    "#### Prompt\n",
    "```\n",
    "A table summarizing the fruits from Goocrux:\n",
    "\n",
    "There are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy. There are also loheckles, which are a grayish blue fruit and are very tart, a little bit like a lemon. Pounits are a bright green color and are more savory than sweet. There are also plenty of loopnovas which are a neon pink flavor and taste like cotton candy. Finally, there are fruits called glowls, which have a very sour and bitter taste which is acidic and caustic, and a pale orange tinge to them.\n",
    "\n",
    "| Fruit | Color | Flavor |\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| --- | --- | --- |\n",
      "| Neoskizzles | Purple | Candy |\n",
      "| Loheckles | Grayish Blue | Tart, like a lemon |\n",
      "| Pounits | Bright Green | Savory |\n",
      "| Loopnovas | Neon Pink | Cotton Candy |\n",
      "| Glowls | Pale Orange | Sour and Bitter, Acidic and Caustic |\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "response = openai.Completion.create(\n",
    "  model=\"text-davinci-003\",\n",
    "  prompt=\"A table summarizing the fruits from Goocrux:\\n\\nThere are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy. There are also loheckles, which are a grayish blue fruit and are very tart, a little bit like a lemon. Pounits are a bright green color and are more savory than sweet. There are also plenty of loopnovas which are a neon pink flavor and taste like cotton candy. Finally, there are fruits called glowls, which have a very sour and bitter taste which is acidic and caustic, and a pale orange tinge to them.\\n\\n| Fruit | Color | Flavor |\",\n",
    "  temperature=0,\n",
    "  max_tokens=100,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0\n",
    ")\n",
    "\n",
    "print(response.choices[0].text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion Example (Translate natural language to SQL queries.)\n",
    "Translate natural language to SQL queries.\n",
    "```\n",
    "#### Prompt\n",
    "### Postgres SQL tables, with their properties:\n",
    "#\n",
    "# Employee(id, name, department_id)\n",
    "# Department(id, name, address)\n",
    "# Salary_Payments(id, employee_id, amount, date)\n",
    "#\n",
    "### A query to list the names of the departments which employed more than 10 employees in the last 3 months\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT d.name \n",
      "FROM Department d \n",
      "INNER JOIN Employee e ON d.id = e.department_id \n",
      "INNER JOIN Salary_Payments sp ON e.id = sp.employee_id \n",
      "WHERE sp.date > NOW() - INTERVAL '3 months' \n",
      "GROUP BY d.name \n",
      "HAVING COUNT(*) > 10\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "response = openai.Completion.create(\n",
    "  model=\"text-davinci-003\",\n",
    "  prompt=\"### Postgres SQL tables, with their properties:\\n#\\n# Employee(id, name, department_id)\\n# Department(id, name, address)\\n# Salary_Payments(id, employee_id, amount, date)\\n#\\n### A query to list the names of the departments which employed more than 10 employees in the last 3 months\\n\",\n",
    "  temperature=0,\n",
    "  max_tokens=150,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=0.0,\n",
    "  stop=[\"#\", \";\"]\n",
    ")\n",
    "\n",
    "print(response.choices[0].text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat\n",
    "\n",
    "### Prompt\n",
    "A prompt refers to the initial input or starting point for generating natural language text using the provided text-generating model.\n",
    "\n",
    "### Context Window\n",
    "\n",
    "a context window is a range of words or characters surrounding a particular word or sequence of words in a text. This window is used to provide additional context for a machine learning model to better understand the meaning of the target word or sequence.\n",
    "\n",
    "For example, consider the sentence \"I love to eat ice cream\". If we want to predict the next word after \"eat\", we can use a context window to provide additional context. A simple context window might be the surrounding words \"love to\" and \"ice cream\". This helps the model understand that \"eat\" in this context is likely associated with food or eating, rather than some other possible meanings of the word.\n",
    "\n",
    "The size of the context window can vary depending on the specific task and model being used. In some cases, a larger context window may be necessary to capture more complex relationships between words, while in other cases a smaller window may be sufficient.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Chat Bot with Context\n",
    "\n",
    "```mermaid\n",
    "sequenceDiagram\n",
    "    participant User\n",
    "    participant Model\n",
    "    User->>Model: Hello, how are you?\n",
    "    Model->>Model: Context window: \"Hello, how are you?\"\n",
    "    Model->>Model: Generating response\n",
    "    Model->>User: I'm doing well, thank you. How can I assist you?\n",
    "    User->>Model: I need help with my account.\n",
    "    Model->>Model: Context window: \"Hello, how are you? I'm doing well, thank you. How can I assist you?\"\n",
    "    Model->>Model: Generating response\n",
    "    Model->>User: Sure, what specifically do you need help with?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "# Set up OpenAI API credentials and model\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "conversation_history = []  # Empty list to store conversation history\n",
    "\n",
    "def chat(user_input: str):\n",
    "    # Example conversation loop\n",
    "    # Get input from user\n",
    "    print(\"User: \" + user_input)\n",
    "\n",
    "    # Append previous conversation to input prompt\n",
    "    prompt = \"Conversation history:\\n\" + \"\\n\".join(conversation_history) + \"\\n\\nUser: \" + user_input\n",
    "\n",
    "    # Generate response from OpenAI model\n",
    "    response = openai.Completion.create(\n",
    "        model=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=1024,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    # Extract response text\n",
    "    ai_response = response.choices[0].text.strip()\n",
    "\n",
    "    # Print AI response\n",
    "    print(\"AI:\", ai_response)\n",
    "\n",
    "    # Add user input and AI response to conversation history\n",
    "    conversation_history.append(\"User: \" + user_input)\n",
    "    conversation_history.append(\"AI: \" + ai_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hello how are you doing?\n",
      "AI: Bot: Hi there! I'm doing great, thanks for asking. How about you?\n"
     ]
    }
   ],
   "source": [
    "chat(\"Hello how are you doing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: I need help with my account\n",
      "AI: AI: What kind of help do you need with your account?\n"
     ]
    }
   ],
   "source": [
    "chat(\"I need help with my account.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we set up a conversation loop that prompts the user for input and generates a response using the OpenAI API. Before sending the prompt to the API, we append the entire conversation history up to that point to the prompt, with the new user input at the end. The AI response is then extracted from the API response and printed to the console, and the user input and AI response are added to the conversation history list."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat API and SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"content\": \"Hello there! How can I assist you today?\",\n",
      "  \"role\": \"assistant\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "  ]\n",
    ")\n",
    " \n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When in the Course of human events, it becomes necessary for one people to dissolve the political bands\n",
      "which have connected them with another, and to assume among the powers of the earth, the separate\n",
      "and equal station to which the Laws of Nature and of Nature's God entitle them, a decent respect to the\n",
      "opinions of mankind requires that they should declare the causes which impel them to the separation.\n",
      "\n",
      "We hold these truths to be self-evident, that all men are created equal, that they are endowed by their Creator\n",
      "with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness.--That to secure\n",
      "these rights, Governments are instituted among Men, deriving their just powers from the consent of the governed,"
     ]
    }
   ],
   "source": [
    "# Import the openai library\n",
    "import openai\n",
    "import os\n",
    "\n",
    "# Set your API key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Define your messages\n",
    "messages = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a history teacher.\"},\n",
    "  {\"role\": \"user\", \"content\": \"Write the first two paragraphs of the Declaration of Independence. Each line should only be 80 characters long.\"}\n",
    "]\n",
    "\n",
    "# Create a chat completion object with stream=True\n",
    "completion = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=messages,\n",
    "  stream=True\n",
    ")\n",
    "\n",
    "# Iterate over the chunks of output\n",
    "for chunk in completion:\n",
    "  # Get the content of the chunk\n",
    "  content = chunk[\"choices\"][0].get(\"delta\", {}).get(\"content\")\n",
    "  # If there is content, print it\n",
    "  if content is not None:\n",
    "    print(content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
